{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "trainingDF = pd.read_csv(\"..\\\\Datasets\\\\train.csv\", encoding = \"utf-8\", low_memory = False)\n",
    "\n",
    "print(trainingDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "testingDF = pd.read_csv(\"..\\\\Datasets\\\\test.csv\", encoding = \"utf-8\", low_memory = False)\n",
    "\n",
    "print(testingDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of NULL values in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(trainingDF.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of NULL values in the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(testingDF.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing 'PassengerId' of testing dataset for later using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDFPassengerId = testingDF[\"PassengerId\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass     Sex   Age  SibSp  Parch Embarked\n",
      "0           0       3    male  22.0      1      0        S\n",
      "1           1       1  female  38.0      1      0        C\n",
      "2           1       3  female  26.0      0      0        S\n",
      "3           1       1  female  35.0      1      0        S\n",
      "4           0       3    male  35.0      0      0        S\n",
      "..        ...     ...     ...   ...    ...    ...      ...\n",
      "886         0       2    male  27.0      0      0        S\n",
      "887         1       1  female  19.0      0      0        S\n",
      "888         0       3  female   NaN      1      2        S\n",
      "889         1       1    male  26.0      0      0        C\n",
      "890         0       3    male  32.0      0      0        Q\n",
      "\n",
      "[891 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "trainingDF = trainingDF.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\"], axis = 1)\n",
    "\n",
    "print(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass     Sex   Age  SibSp  Parch Embarked\n",
      "0         3    male  34.5      0      0        Q\n",
      "1         3  female  47.0      1      0        S\n",
      "2         2    male  62.0      0      0        Q\n",
      "3         3    male  27.0      0      0        S\n",
      "4         3  female  22.0      1      1        S\n",
      "..      ...     ...   ...    ...    ...      ...\n",
      "413       3    male   NaN      0      0        S\n",
      "414       1  female  39.0      0      0        C\n",
      "415       3    male  38.5      0      0        S\n",
      "416       3    male   NaN      0      0        S\n",
      "417       3    male   NaN      1      1        C\n",
      "\n",
      "[418 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "testingDF = testingDF.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\"], axis = 1)\n",
    "\n",
    "print(testingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing NULL values in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categoricalImputer = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\n",
    "numericalImputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "\n",
    "trainingDF[\"Age\"] = numericalImputer.fit_transform(trainingDF[\"Age\"].values.reshape(-1, 1)).flatten()\n",
    "# trainingDF[\"Cabin\"] = categoricalImputer.fit_transform(trainingDF[\"Cabin\"].values.reshape(-1, 1)).flatten()\n",
    "trainingDF[\"Embarked\"] = categoricalImputer.fit_transform(trainingDF[\"Embarked\"].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing NULL values in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categoricalImputer = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\n",
    "numericalImputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "\n",
    "testingDF[\"Age\"] = numericalImputer.fit_transform(testingDF[\"Age\"].values.reshape(-1, 1)).flatten()\n",
    "# testingDF[\"Cabin\"] = categoricalImputer.fit_transform(testingDF[\"Cabin\"].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values in training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(trainingDF.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(testingDF.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the features and target variables from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trainingDF.iloc[:, 1:].values\n",
    "y_train = trainingDF.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 'male' 22.0 1 0 'S']\n",
      " [1 'female' 38.0 1 0 'C']\n",
      " [3 'female' 26.0 0 0 'S']\n",
      " ...\n",
      " [3 'female' 29.69911764705882 1 2 'S']\n",
      " [1 'male' 26.0 0 0 'C']\n",
      " [3 'male' 32.0 0 0 'Q']]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
      " 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
      " 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting features to training dataset to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = testingDF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 'male' 34.5 0 0 'Q']\n",
      " [3 'female' 47.0 1 0 'S']\n",
      " [2 'male' 62.0 0 0 'Q']\n",
      " ...\n",
      " [3 'male' 38.5 0 0 'S']\n",
      " [3 'male' 30.272590361445783 0 0 'S']\n",
      " [3 'male' 30.272590361445783 1 1 'C']]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding Sex column in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "x_train[:, 1] = le.fit_transform(x_train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 22.0 1 0 'S']\n",
      " [1 0 38.0 1 0 'C']\n",
      " [3 0 26.0 0 0 'S']\n",
      " ...\n",
      " [3 0 29.69911764705882 1 2 'S']\n",
      " [1 1 26.0 0 0 'C']\n",
      " [3 1 32.0 0 0 'Q']]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding Sex column in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "x_test[:, 1] = le.fit_transform(x_test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 34.5 0 0 'Q']\n",
      " [3 0 47.0 1 0 'S']\n",
      " [2 1 62.0 0 0 'Q']\n",
      " ...\n",
      " [3 1 38.5 0 0 'S']\n",
      " [3 1 30.272590361445783 0 0 'S']\n",
      " [3 1 30.272590361445783 1 1 'C']]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding the Cabin and Embarked columns in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers = [(\"encoder_embarked\", OneHotEncoder(), [5])], remainder = \"passthrough\")\n",
    "x_train = np.array(ct.fit_transform(x_train))\n",
    "\n",
    "# x_train = x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 ... 22.0 1 0]\n",
      " [1.0 0.0 0.0 ... 38.0 1 0]\n",
      " [0.0 0.0 1.0 ... 26.0 0 0]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 29.69911764705882 1 2]\n",
      " [1.0 0.0 0.0 ... 26.0 0 0]\n",
      " [0.0 1.0 0.0 ... 32.0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding the Cabin and Embarked columns in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers = [(\"encoder_embarked\", OneHotEncoder(), [5])], remainder = \"passthrough\")\n",
    "x_test = np.array(ct.fit_transform(x_test))\n",
    "\n",
    "# x_test = x_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 ... 34.5 0 0]\n",
      " [0.0 0.0 1.0 ... 47.0 1 0]\n",
      " [0.0 1.0 0.0 ... 62.0 0 0]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 38.5 0 0]\n",
      " [0.0 0.0 1.0 ... 30.272590361445783 0 0]\n",
      " [1.0 0.0 0.0 ... 30.272590361445783 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression model on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MVIKRAMS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state = 42)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the testing dataset values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 1 1 1 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump result to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 892    0]\n",
      " [ 893    0]\n",
      " [ 894    0]\n",
      " [ 895    0]\n",
      " [ 896    1]\n",
      " [ 897    0]\n",
      " [ 898    1]\n",
      " [ 899    0]\n",
      " [ 900    1]\n",
      " [ 901    0]\n",
      " [ 902    0]\n",
      " [ 903    0]\n",
      " [ 904    1]\n",
      " [ 905    0]\n",
      " [ 906    1]\n",
      " [ 907    1]\n",
      " [ 908    0]\n",
      " [ 909    0]\n",
      " [ 910    1]\n",
      " [ 911    1]\n",
      " [ 912    0]\n",
      " [ 913    0]\n",
      " [ 914    1]\n",
      " [ 915    1]\n",
      " [ 916    1]\n",
      " [ 917    0]\n",
      " [ 918    1]\n",
      " [ 919    0]\n",
      " [ 920    0]\n",
      " [ 921    0]\n",
      " [ 922    0]\n",
      " [ 923    0]\n",
      " [ 924    0]\n",
      " [ 925    0]\n",
      " [ 926    1]\n",
      " [ 927    0]\n",
      " [ 928    1]\n",
      " [ 929    1]\n",
      " [ 930    0]\n",
      " [ 931    0]\n",
      " [ 932    0]\n",
      " [ 933    0]\n",
      " [ 934    0]\n",
      " [ 935    1]\n",
      " [ 936    1]\n",
      " [ 937    0]\n",
      " [ 938    0]\n",
      " [ 939    0]\n",
      " [ 940    1]\n",
      " [ 941    0]\n",
      " [ 942    0]\n",
      " [ 943    0]\n",
      " [ 944    1]\n",
      " [ 945    1]\n",
      " [ 946    0]\n",
      " [ 947    0]\n",
      " [ 948    0]\n",
      " [ 949    0]\n",
      " [ 950    0]\n",
      " [ 951    1]\n",
      " [ 952    0]\n",
      " [ 953    0]\n",
      " [ 954    0]\n",
      " [ 955    1]\n",
      " [ 956    1]\n",
      " [ 957    1]\n",
      " [ 958    1]\n",
      " [ 959    0]\n",
      " [ 960    1]\n",
      " [ 961    1]\n",
      " [ 962    1]\n",
      " [ 963    0]\n",
      " [ 964    1]\n",
      " [ 965    1]\n",
      " [ 966    1]\n",
      " [ 967    1]\n",
      " [ 968    0]\n",
      " [ 969    1]\n",
      " [ 970    0]\n",
      " [ 971    1]\n",
      " [ 972    0]\n",
      " [ 973    0]\n",
      " [ 974    0]\n",
      " [ 975    0]\n",
      " [ 976    0]\n",
      " [ 977    0]\n",
      " [ 978    1]\n",
      " [ 979    1]\n",
      " [ 980    1]\n",
      " [ 981    0]\n",
      " [ 982    1]\n",
      " [ 983    0]\n",
      " [ 984    1]\n",
      " [ 985    0]\n",
      " [ 986    1]\n",
      " [ 987    0]\n",
      " [ 988    1]\n",
      " [ 989    0]\n",
      " [ 990    1]\n",
      " [ 991    0]\n",
      " [ 992    1]\n",
      " [ 993    0]\n",
      " [ 994    0]\n",
      " [ 995    0]\n",
      " [ 996    1]\n",
      " [ 997    0]\n",
      " [ 998    0]\n",
      " [ 999    0]\n",
      " [1000    0]\n",
      " [1001    0]\n",
      " [1002    0]\n",
      " [1003    1]\n",
      " [1004    1]\n",
      " [1005    1]\n",
      " [1006    1]\n",
      " [1007    0]\n",
      " [1008    0]\n",
      " [1009    1]\n",
      " [1010    1]\n",
      " [1011    1]\n",
      " [1012    1]\n",
      " [1013    0]\n",
      " [1014    1]\n",
      " [1015    0]\n",
      " [1016    0]\n",
      " [1017    1]\n",
      " [1018    0]\n",
      " [1019    1]\n",
      " [1020    0]\n",
      " [1021    0]\n",
      " [1022    0]\n",
      " [1023    0]\n",
      " [1024    1]\n",
      " [1025    0]\n",
      " [1026    0]\n",
      " [1027    0]\n",
      " [1028    0]\n",
      " [1029    0]\n",
      " [1030    1]\n",
      " [1031    0]\n",
      " [1032    0]\n",
      " [1033    1]\n",
      " [1034    0]\n",
      " [1035    0]\n",
      " [1036    0]\n",
      " [1037    0]\n",
      " [1038    0]\n",
      " [1039    0]\n",
      " [1040    0]\n",
      " [1041    0]\n",
      " [1042    1]\n",
      " [1043    0]\n",
      " [1044    0]\n",
      " [1045    0]\n",
      " [1046    0]\n",
      " [1047    0]\n",
      " [1048    1]\n",
      " [1049    1]\n",
      " [1050    0]\n",
      " [1051    1]\n",
      " [1052    1]\n",
      " [1053    0]\n",
      " [1054    1]\n",
      " [1055    0]\n",
      " [1056    0]\n",
      " [1057    1]\n",
      " [1058    0]\n",
      " [1059    0]\n",
      " [1060    1]\n",
      " [1061    1]\n",
      " [1062    0]\n",
      " [1063    0]\n",
      " [1064    0]\n",
      " [1065    0]\n",
      " [1066    0]\n",
      " [1067    1]\n",
      " [1068    1]\n",
      " [1069    0]\n",
      " [1070    1]\n",
      " [1071    1]\n",
      " [1072    0]\n",
      " [1073    0]\n",
      " [1074    1]\n",
      " [1075    0]\n",
      " [1076    1]\n",
      " [1077    0]\n",
      " [1078    1]\n",
      " [1079    0]\n",
      " [1080    0]\n",
      " [1081    0]\n",
      " [1082    0]\n",
      " [1083    0]\n",
      " [1084    0]\n",
      " [1085    0]\n",
      " [1086    0]\n",
      " [1087    0]\n",
      " [1088    1]\n",
      " [1089    1]\n",
      " [1090    0]\n",
      " [1091    1]\n",
      " [1092    1]\n",
      " [1093    0]\n",
      " [1094    0]\n",
      " [1095    1]\n",
      " [1096    0]\n",
      " [1097    1]\n",
      " [1098    1]\n",
      " [1099    0]\n",
      " [1100    1]\n",
      " [1101    0]\n",
      " [1102    0]\n",
      " [1103    0]\n",
      " [1104    0]\n",
      " [1105    0]\n",
      " [1106    0]\n",
      " [1107    0]\n",
      " [1108    1]\n",
      " [1109    0]\n",
      " [1110    1]\n",
      " [1111    0]\n",
      " [1112    1]\n",
      " [1113    0]\n",
      " [1114    1]\n",
      " [1115    0]\n",
      " [1116    1]\n",
      " [1117    1]\n",
      " [1118    0]\n",
      " [1119    1]\n",
      " [1120    0]\n",
      " [1121    0]\n",
      " [1122    0]\n",
      " [1123    1]\n",
      " [1124    0]\n",
      " [1125    0]\n",
      " [1126    0]\n",
      " [1127    0]\n",
      " [1128    0]\n",
      " [1129    0]\n",
      " [1130    1]\n",
      " [1131    1]\n",
      " [1132    1]\n",
      " [1133    1]\n",
      " [1134    0]\n",
      " [1135    0]\n",
      " [1136    0]\n",
      " [1137    0]\n",
      " [1138    1]\n",
      " [1139    0]\n",
      " [1140    1]\n",
      " [1141    1]\n",
      " [1142    1]\n",
      " [1143    0]\n",
      " [1144    1]\n",
      " [1145    0]\n",
      " [1146    0]\n",
      " [1147    0]\n",
      " [1148    0]\n",
      " [1149    0]\n",
      " [1150    1]\n",
      " [1151    0]\n",
      " [1152    0]\n",
      " [1153    0]\n",
      " [1154    1]\n",
      " [1155    1]\n",
      " [1156    0]\n",
      " [1157    0]\n",
      " [1158    0]\n",
      " [1159    0]\n",
      " [1160    1]\n",
      " [1161    0]\n",
      " [1162    0]\n",
      " [1163    0]\n",
      " [1164    1]\n",
      " [1165    1]\n",
      " [1166    0]\n",
      " [1167    1]\n",
      " [1168    0]\n",
      " [1169    0]\n",
      " [1170    0]\n",
      " [1171    0]\n",
      " [1172    1]\n",
      " [1173    0]\n",
      " [1174    1]\n",
      " [1175    1]\n",
      " [1176    1]\n",
      " [1177    0]\n",
      " [1178    0]\n",
      " [1179    0]\n",
      " [1180    0]\n",
      " [1181    0]\n",
      " [1182    0]\n",
      " [1183    1]\n",
      " [1184    0]\n",
      " [1185    0]\n",
      " [1186    0]\n",
      " [1187    0]\n",
      " [1188    1]\n",
      " [1189    0]\n",
      " [1190    0]\n",
      " [1191    0]\n",
      " [1192    0]\n",
      " [1193    0]\n",
      " [1194    0]\n",
      " [1195    0]\n",
      " [1196    1]\n",
      " [1197    1]\n",
      " [1198    0]\n",
      " [1199    0]\n",
      " [1200    0]\n",
      " [1201    0]\n",
      " [1202    0]\n",
      " [1203    0]\n",
      " [1204    0]\n",
      " [1205    1]\n",
      " [1206    1]\n",
      " [1207    1]\n",
      " [1208    0]\n",
      " [1209    0]\n",
      " [1210    0]\n",
      " [1211    0]\n",
      " [1212    0]\n",
      " [1213    0]\n",
      " [1214    0]\n",
      " [1215    0]\n",
      " [1216    1]\n",
      " [1217    0]\n",
      " [1218    1]\n",
      " [1219    0]\n",
      " [1220    0]\n",
      " [1221    0]\n",
      " [1222    1]\n",
      " [1223    1]\n",
      " [1224    0]\n",
      " [1225    1]\n",
      " [1226    0]\n",
      " [1227    0]\n",
      " [1228    0]\n",
      " [1229    0]\n",
      " [1230    0]\n",
      " [1231    0]\n",
      " [1232    0]\n",
      " [1233    0]\n",
      " [1234    0]\n",
      " [1235    1]\n",
      " [1236    0]\n",
      " [1237    1]\n",
      " [1238    0]\n",
      " [1239    1]\n",
      " [1240    0]\n",
      " [1241    1]\n",
      " [1242    1]\n",
      " [1243    0]\n",
      " [1244    0]\n",
      " [1245    0]\n",
      " [1246    1]\n",
      " [1247    0]\n",
      " [1248    1]\n",
      " [1249    0]\n",
      " [1250    0]\n",
      " [1251    0]\n",
      " [1252    0]\n",
      " [1253    1]\n",
      " [1254    1]\n",
      " [1255    0]\n",
      " [1256    1]\n",
      " [1257    0]\n",
      " [1258    0]\n",
      " [1259    1]\n",
      " [1260    1]\n",
      " [1261    0]\n",
      " [1262    0]\n",
      " [1263    1]\n",
      " [1264    0]\n",
      " [1265    0]\n",
      " [1266    1]\n",
      " [1267    1]\n",
      " [1268    0]\n",
      " [1269    0]\n",
      " [1270    0]\n",
      " [1271    0]\n",
      " [1272    0]\n",
      " [1273    0]\n",
      " [1274    1]\n",
      " [1275    1]\n",
      " [1276    0]\n",
      " [1277    1]\n",
      " [1278    0]\n",
      " [1279    0]\n",
      " [1280    0]\n",
      " [1281    0]\n",
      " [1282    1]\n",
      " [1283    1]\n",
      " [1284    0]\n",
      " [1285    0]\n",
      " [1286    0]\n",
      " [1287    1]\n",
      " [1288    0]\n",
      " [1289    1]\n",
      " [1290    0]\n",
      " [1291    0]\n",
      " [1292    1]\n",
      " [1293    0]\n",
      " [1294    1]\n",
      " [1295    1]\n",
      " [1296    0]\n",
      " [1297    0]\n",
      " [1298    0]\n",
      " [1299    0]\n",
      " [1300    1]\n",
      " [1301    1]\n",
      " [1302    1]\n",
      " [1303    1]\n",
      " [1304    1]\n",
      " [1305    0]\n",
      " [1306    1]\n",
      " [1307    0]\n",
      " [1308    0]\n",
      " [1309    0]]\n"
     ]
    }
   ],
   "source": [
    "result = np.concatenate((testingDFPassengerId.reshape(len(testingDFPassengerId), 1), y_pred.reshape(len(y_pred), 1)), 1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pd.DataFrame(result, columns = [\"PassengerId\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(resultDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.to_csv(\"..\\\\Datasets\\\\survived_solution.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
